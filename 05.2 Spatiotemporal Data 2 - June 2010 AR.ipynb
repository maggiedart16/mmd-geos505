{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4098fadc",
   "metadata": {},
   "source": [
    "## 05.2 Spatiotemporal Data 2: Multifile Datasets\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Often times, it's the case that our spatiotemporal datasets are comprised not of 1 file, but of many. Specific examples where this is true include:\n",
    "\n",
    "1. Time series of remote sensing data, where the data is released on the same geospatial footprint at specified intervals (e.g., Landsat row and path footprints),\n",
    "2. Weather forecasting data where each file may be a forecast for a set lead time into the future from a particular initial starting point, \n",
    "3. Climate projection data where each file may contain a year or even a decade of climate information under some given scenario,\n",
    "4. Output of some kind of geophysical model like an ice sheet transport, hydrologic, or snow accumulation and melt model. \n",
    "\n",
    "We also previously saw that the NetCDF file format conveys a lot of advantages because we can store many different variables in the same file, as long as the variables can be described using one or more of the dimensions and coordinate axes describing the file. A double-edged sword of this flexibility, however, lies in the fact that our files can grow very large, very quickly. As I mentioned in class, __one day (24 hours)__ of raw simulated WRF output for our southern Idaho domain produces a single NetCDF file that is 15 GB in size – or about 5.5 TB for a full year of simulation. This predicament leads us to the following sets of questions:\n",
    "\n",
    "- How can we analyze a dataset that is comprised of multiple files, as if it were a single dataset?\n",
    "- Is there a way to analyze multifile datasets, when the sum total of all of the files in the dataet exceed our RAM for our individual machine?\n",
    "\n",
    "Thankfully, `xArray` provides us an answer to both of the above questions. First some background on the data we'll be using today, and the event we'll be investigating. \n",
    "\n",
    "By late May 2010, the water year in the Boise and Payette River basins was shaping up to be great in terms of water resources. Reservoirs were almost full and there was still significant snow at the higher elevations of many water supply basins. However, in early June, a late atmospheric river (AR) would dump significant rain over southwest Idaho, melting a lot of snow in the upper basins and causing rivers to flood. Check out the summary of this event, on the Boise NWS website: [https://www.weather.gov/safety/flood-states-id](https://www.weather.gov/safety/flood-states-id). \n",
    "\n",
    "Many historical weather and climate datasets are not sufficiently fine in spatial or temporal resolutions to be able to analyze the hour-to-hour evolution of this event. As such, we have trouble asking and answering questions like \"how did the rain-snow transition evolve over the course of this storm?\" or \"how did peak-hourly precipitation during the storm vary in space?\" Today, you will be an early adopter of NASA's North American Land Data Assimilation, version 3 (NLDAS-3) data, a new dataset that provides estimates of hydrometeorological variables at 1 km, 1 hr spatiotemporal resolution over all of North America by combining physics based models with available remote sensing data. Read more about NLDAS-3 data here: [https://ldas.gsfc.nasa.gov/nldas/v3](https://ldas.gsfc.nasa.gov/nldas/v3)\n",
    "\n",
    "This notebook uses NLDAS-3 hydrometeorological forcing data to analyze climate conditions in May and June 2010 and assess what fraction of precipitation that fell during those months was associated with this lone AR event. \n",
    "\n",
    "### 2. Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445b7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Needed for numerical operations\n",
    "import matplotlib.pyplot as plt # Needed for plotting\n",
    "import xarray as xr # Needed for handling NetCDF data\n",
    "import cartopy.crs as ccrs # Needed for cartographic projections\n",
    "import cartopy.feature as cfeature # Needed for cartographic features\n",
    "\n",
    "# Output file parameters\n",
    "nldas3_dir = 'C:/Users/maggiedart/Documents/mmd-geos505/ubrb_nldas3_data-20251208T204014Z-3-001' # Directory for NLDAS-3 data\n",
    "nldas3_filebase = 'nldas3_UBRB_subset_' # Base name for NLDAS-3 files\n",
    "nldas3_fileext = '.nc' # File extension for NLDAS-3 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469ecf3",
   "metadata": {},
   "source": [
    "### 3. Load the Data\n",
    "\n",
    "Amazingly, `xArray` can load a multiple-file dataset with a single line of code. We just need to make sure we tell `xArray` how the multiple files in our dataset should be combined. Let's examine what the \"loaded\" dataset looks like and talk about a few things that distinguish the dataset from the underlying files. Specifically, try to find something called \"chunksize\" and see if you can surmise what it might mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef92d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m nldas3_ubrb_ds = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnldas3_dir\u001b[49m\u001b[43m+\u001b[49m\u001b[43mnldas3_filebase\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mnldas3_fileext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mby_coords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m nldas3_ubrb_ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maggiedart\\AppData\\Local\\miniconda3\\envs\\geos505\\Lib\\site-packages\\xarray\\backends\\api.py:1577\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, errors, **kwargs)\u001b[39m\n\u001b[32m   1574\u001b[39m paths = _find_absolute_paths(paths, engine=engine, **kwargs)\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[32m-> \u001b[39m\u001b[32m1577\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mno files to open\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1579\u001b[39m paths1d: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m | ReadBuffer]\n\u001b[32m   1580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m combine == \u001b[33m\"\u001b[39m\u001b[33mnested\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOSError\u001b[39m: no files to open"
     ]
    }
   ],
   "source": [
    "nldas3_ubrb_ds = xr.open_mfdataset(nldas3_dir+nldas3_filebase+'*'+nldas3_fileext, combine='by_coords')\n",
    "nldas3_ubrb_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e7016",
   "metadata": {},
   "source": [
    "### 4. Plot the Total Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f801b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nldas3_ubrb_ds['Rainf'].sum(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d040e66",
   "metadata": {},
   "source": [
    "### 5. Isolate AR and non-AR Periods for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_ds = nldas3_ubrb_ds.sel(time=slice('2010-06-02','2010-06-04'))\n",
    "ar_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_ds['Rainf'].sum(dim='time').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40012b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_before_ds = nldas3_ubrb_ds.sel(time=slice('2010-05-01','2010-06-01'))\n",
    "ar_after_ds = nldas3_ubrb_ds.sel(time=slice('2010-06-05','2010-06-30'))\n",
    "\n",
    "prcp_no_ar = ar_before_ds['Rainf'].sum(dim='time') + ar_after_ds['Rainf'].sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prcp_no_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prcp_ar = ar_ds['Rainf'].sum(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fa648",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_frac = prcp_ar / prcp_no_ar\n",
    "\n",
    "ar_frac.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722419a0",
   "metadata": {},
   "source": [
    "### 6. Create Maps with Spatial Context Using `Cartopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ccrs.PlateCarree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12f23e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m states = cfeature.NaturalEarthFeature(\n\u001b[32m      2\u001b[39m     category=\u001b[33m'\u001b[39m\u001b[33mcultural\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33madmin_1_states_provinces_lines\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     scale=\u001b[33m'\u001b[39m\u001b[33m50m\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     facecolor=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m rivers = cfeature.NaturalEarthFeature(\n\u001b[32m      9\u001b[39m     category=\u001b[33m'\u001b[39m\u001b[33mphysical\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mrivers_lake_centerlines\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     scale=\u001b[33m'\u001b[39m\u001b[33m50m\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     facecolor=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m fig, ax = plt.subplots(figsize=(\u001b[32m14\u001b[39m, \u001b[32m9\u001b[39m), subplot_kw={\u001b[33m'\u001b[39m\u001b[33mprojection\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mprojection\u001b[49m})\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#ax.contourf(tot_prcp.x, tot_prcp.y, tot_prcp, transform=projection)\u001b[39;00m\n\u001b[32m     17\u001b[39m ax.add_feature(states, edgecolor=\u001b[33m'\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m'\u001b[39m, linewidth=\u001b[32m0.7\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'projection' is not defined"
     ]
    }
   ],
   "source": [
    "states = cfeature.NaturalEarthFeature(\n",
    "    category='cultural',\n",
    "    name='admin_1_states_provinces_lines',\n",
    "    scale='50m',\n",
    "    facecolor='none'\n",
    ")\n",
    "\n",
    "rivers = cfeature.NaturalEarthFeature(\n",
    "    category='physical',\n",
    "    name='rivers_lake_centerlines',\n",
    "    scale='50m',\n",
    "    facecolor='none'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 9), subplot_kw={'projection': projection})\n",
    "# Add features\n",
    "ax.add_feature(states, edgecolor='black', linewidth=0.7)\n",
    "ax.add_feature(rivers, edgecolor='blue', linewidth=1.0)\n",
    "ax.add_feature(cfeature.BORDERS, edgecolor='black', linewidth=1.0)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=1.0)\n",
    "\n",
    "im = ax.contourf(ar_frac['lon'], ar_frac['lat'], ar_frac, transform=projection)\n",
    "\n",
    "\n",
    "plt.colorbar(im, ax=ax, orientation='vertical', label='AR Fraction of Total Precipitation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geos505",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
